{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6d34fe-9cc5-404f-b01b-5000039f0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077c6d79-bf34-4f1d-88b5-0885d3339c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 620 MB.\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    # For GPU 0\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    memory_used = info.used\n",
    "    # For GPU 1\n",
    "    handle = nvmlDeviceGetHandleByIndex(1)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    memory_used += info.used\n",
    "    print(f\"GPU memory occupied: {memory_used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e3e321-33e3-48da-8811-934ddadd5d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/accelerate/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 2564 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\").to('cuda:0')\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a6bf17-8691-43cc-945a-11436b23d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    \"output_dir\": \"outputs\",\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0c742c-752e-4944-9d28-307efcc0a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 1, (dataset_size)),\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09889b6e-e50c-41f4-b6f5-1fddbe2e420b",
   "metadata": {},
   "source": [
    "# Mixed Precision Training\n",
    "Let’s have a look at another method with which we can regain some speed: mixed precision training. The idea of mixed precision training is that **not all variables need to be stored in full (32-bit) floating point precision**. **If we can reduce the precision, the variables and their computations are faster.**\n",
    "\n",
    "Here are the commonly used floating point data types choice of which impacts both memory usage and throughput:\n",
    "\n",
    "* fp32 (float32)\n",
    "* fp16 (float16)\n",
    "* bf16 (bfloat16)\n",
    "* tf32 (CUDA internal data type)\n",
    "\n",
    "While fp16 and fp32 have been around for quite some time, bf16 and tf32 are only available on the Ampere architecture GPUS and TPUs support bf16 as well. Let’s start with the most commonly used method which is **FP16 training**.\n",
    "\n",
    "**Although the gradients are also computed in half precision, they are converted back to full precision for the optimization step, so no memory is saved here.**\n",
    "\n",
    "Since the **model is present on the GPU in both 16-bit and 32-bit precision**, this can use **more GPU memory (1.5x the original model is on the GPU)**, especially for small batch sizes.\n",
    "\n",
    "The main advantage comes from **saving the activations in half (16-bit) precision**. \n",
    "\n",
    "Since some computations are performed in full and some in half precision, this approach is called mixed precision training. Enabling mixed precision training is also just a matter of setting the <code>fp16</code> flag to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f2e9e8-ab7a-4ac5-8ae8-92998f9d18d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/accelerate/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 00:22, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 23.46\n",
      "Samples/second: 21.82\n",
      "GPU memory occupied: 11224 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4, \n",
    "    fp16=True, \n",
    "    **default_args\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=ds\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af50335-9959-4758-9c3f-ce98e929929b",
   "metadata": {},
   "source": [
    "Here we compare the results with the vanilla training.\n",
    "\n",
    "* Time: 40.96 **(Decrease to 23.46 seconds)**\n",
    "* Samples/second: 12.50 **(Increase to 21.82 samples per second)**\n",
    "* GPU memory occupied: 12852 MB. **(Decrease to 11224 MB)**\n",
    "\n",
    "We can see that this is almost twice as fast as the vanilla training.\n",
    "\n",
    "Next, let’s add the mixed precision training to the previous methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46976616-5759-47ad-bdd2-7ad36ea88955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/accelerate/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 00:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39.06\n",
      "Samples/second: 13.11\n",
      "GPU memory occupied: 7574 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    **default_args,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f367d5b5-21f5-4720-9136-716dd87b1af2",
   "metadata": {},
   "source": [
    "Here we compare the results with the one with only fp16 enabled:\n",
    "\n",
    "* Time: 23.46 (Increase to 39.06 seconds)\n",
    "* Samples/second: 21.82 (Decrease to 13.11 samples per second)\n",
    "* GPU memory occupied: 11224 MB **(Decrease to 7574 MB)**\n",
    "\n",
    "Here we compare the results with the one only enabing gradient accumulation and gradient checkpointing:\n",
    "\n",
    "* Time: 63.28 **(Decrease to 39.06 seconds)**\n",
    "* Samples/second: 8.09 **(Increase to 13.11 samples per second)**\n",
    "* GPU memory occupied: 7078 MB (Increase to 7574 MB)\n",
    "\n",
    "We can see that with these methods we use about **half the GPU memory** as at the beginning while also being **slightly faster**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
