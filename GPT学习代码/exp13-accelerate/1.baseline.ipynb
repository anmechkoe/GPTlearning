{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4017fd39-d9d8-4384-96e1-d47eab6c6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc3013-e5ba-45cd-83c3-3d6742f92570",
   "metadata": {},
   "source": [
    "# Performance and Scalability\n",
    "Training larger and larger transformer models and deploying them to production comes with a range of challenges. During training, your model can **require more GPU memory than is available** or be **very slow to train**, and when you deploy it for inference, it can be **overwhelmed with the throughput** that is required in the production environment.\n",
    "\n",
    "## Efficient Training on a Single GPU\n",
    "In this section we have a look at a few tricks to **reduce the memory footprint** and **speed up training** for large models and how they are integrated in the Trainer and Accelerate.\n",
    "\n",
    "|**Method**|**Speed**|**Memory**|\n",
    "|:-:|:-:|:-:|\n",
    "|Gradient accumulation|No|Yes|\n",
    "|Gradient checkpointing|No|Yes|\n",
    "|Mixed precision training|Yes|No|\n",
    "|Batch size|Yes|Yes|\n",
    "|Optimizer choice|Yes|Yes|\n",
    "|DataLoader|Yes|No|\n",
    "|DeepSpeed Zero|No|Yes|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b884f5-f5f4-4e1d-af2d-91857058bd75",
   "metadata": {},
   "source": [
    "First we setup two helper functions to print summary statistics for the GPU utilization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415ed12e-b6a8-4d82-86e3-b5971b252c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 620 MB.\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    # For GPU 0\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    memory_used = info.used\n",
    "    # For GPU 1\n",
    "    handle = nvmlDeviceGetHandleByIndex(1)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    memory_used += info.used\n",
    "    print(f\"GPU memory occupied: {memory_used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740dd777-d3dd-45fe-bdc8-dacdffba5c7a",
   "metadata": {},
   "source": [
    "That looks good: the GPU memory is not occupied as we would expect before we load any models. If that’s not the case on your machine make sure to stop all processes that are using GPU memory.\n",
    "\n",
    "However, not all free GPU memory can be used by the user. When a model is loaded to the GPU also the kernels are loaded which can take up 1-2GB of memory. To see how much it is we load a tiny tensor into the GPU which triggers the kernels to be loaded as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d683d67-5c6e-45d1-b226-c1563022352f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1276 MB.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.ones((1, 1)).to(\"cuda:0\")\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e9cfcf-2fb2-4db8-80b9-15626ecef7ac",
   "metadata": {},
   "source": [
    "We see that the kernels alone take up ~650 MB of GPU memory. Now let’s see how much space the model uses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07486cc6-b29e-4131-acf9-3571be4eb97c",
   "metadata": {},
   "source": [
    "First, we load the <code>bert-large-uncased</code> model. We load the model weights directly to the GPU so that we can check how much space just weights use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3407cac2-8190-423f-9394-0e897dfe61f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/accelerate/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 2564 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\").to('cuda:0')\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84977ad-39a6-4da7-9fc7-609e9a9a59ae",
   "metadata": {},
   "source": [
    "We can see that the model weights alone take up $(2564 - 1276) / 1024 \\approx 1.3$ GB of the GPU memory.\n",
    "\n",
    "Now we can start training the model and see how the GPU memory consumption changes. First, we set up a few standard training arguments that we will use across all our experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c846b25-47c2-439f-9a1e-67031167c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    \"output_dir\": \"outputs\",\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c42b69-eadb-422d-b9ae-c25f10977dbc",
   "metadata": {},
   "source": [
    "Then we create some dummy data. We create random token IDs between 100 and 30000 and binary labels for a classifier. In total we get 512 sequences each with length 512 and store them in a Dataset with PyTorch format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb63ac7-185b-4a35-9993-d4e417186082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 1, (dataset_size)),\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe823940-1349-4104-8104-7f2c30e94233",
   "metadata": {},
   "source": [
    "As a first experiment we will use the Trainer and train the model without any further modifications and a batch size of 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9369e1f-5271-42d6-86d8-39720aa97f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/accelerate/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 40.9582, 'train_samples_per_second': 12.501, 'train_steps_per_second': 3.125, 'train_loss': 0.03662079945206642, 'epoch': 1.0}\n",
      "Time: 40.96\n",
      "Samples/second: 12.50\n",
      "GPU memory occupied: 12852 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4, \n",
    "    **default_args\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=ds\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb770f4f-1fe6-42dc-9cd7-175cae419ecb",
   "metadata": {},
   "source": [
    "We see that already a relatively small batch size 4 almost fills up our GPU’s memory. However, a larger batch size can often result in faster model convergence or better performance. So ideally we want to tune the batch size to our model’s needs and not to the GPU limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa22e20-fc5b-44d3-b5aa-ddd9f251eb7b",
   "metadata": {},
   "source": [
    "## Anatomy of Model's Operations\n",
    "What’s interesting is that we use much more memory than the size of the model. To understand a bit better why this is the case let’s have look at a model’s operations and memory needs.\n",
    "\n",
    "Transformers architecture includes 3 main groups of operations grouped below by compute-intensity.\n",
    "\n",
    "### Tensor Contractions\n",
    "Linear layers and components of Multi-Head Attention all do **batched matrix-matrix multiplications**. These operations are the most compute-intensive part of training a transformer.\n",
    "\n",
    "### Statistical Normalizations\n",
    "Softmax and layer normalization are less compute-intensive, and involve one or more **reduction operations**, the result of which is then applied via a map.\n",
    "\n",
    "### Element-wise Operators\n",
    "These are the remaining operators: **biases, dropout, activations, and residual connections**. These are the least compute-intensive operations.\n",
    "\n",
    "This knowledge can be helpful to know when analyzing performance bottlenecks.\n",
    "\n",
    "## Anatomy of Model's Memory\n",
    "We've seen that training the model uses much more memory than just putting the model on the GPU. This is because there are many components during training that use GPU memory.\n",
    "\n",
    "The components on GPU memory are the following: \n",
    "1. **model weights**\n",
    "   * 4 bytes * number of parameters for fp32 training\n",
    "   * 6 bytes * number of parameters for **mixed precision training** (maintains a model in fp32 and one in fp16 in memory)\n",
    "2. **optimizer states**\n",
    "   * 8 bytes * number of parameters for normal AdamW (maintains 2 states)\n",
    "   * 2 bytes * number of parameters for 8-bit AdamW optimizers like [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)\n",
    "   * 4 bytes * number of parameters for optimizers like SGD with momentum (maintains only 1 state)\n",
    "3. **gradients**\n",
    "   * 4 bytes * number of parameters for either fp32 or mixed precision training (gradients are always kept in fp32)\n",
    "4. **forward activations** (saved for gradient computation)\n",
    "   * size depends on many factors, the key ones being sequence length, hidden size and batch size.\n",
    "5. **temporary buffers**\n",
    "   * Temporary variables will be released once the calculation is done, but in the moment they could require additional memory and push to out of cuda memory. Therefore explicitly free them as soon as they are no longer needed is crucial.\n",
    "6. **functionality-specific memory**\n",
    "   * The developer software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs.\n",
    "\n",
    "So there are potentially a few places where we could save GPU memory or speed up operations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
