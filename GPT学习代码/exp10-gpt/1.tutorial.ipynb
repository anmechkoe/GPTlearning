{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d3ea969-cee8-4bcb-bb65-528f47be5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ad91e4",
   "metadata": {},
   "source": [
    "è¿™æ®µä»£ç å¯¼å…¥äº†PyTorchåº“å’ŒTransformersåº“ä¸­çš„pipelineæ¨¡å—ã€‚Transformersåº“æ˜¯ä¸€ä¸ªæµè¡Œçš„ç”¨äºŽè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡çš„åº“ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»ã€å‘½åå®žä½“è¯†åˆ«å’Œé—®ç­”ç­‰ã€‚\n",
    "\n",
    "pipelineæ¨¡å—æä¾›äº†ä¸€ä¸ªç®€å•çš„APIï¼Œç”¨äºŽä½¿ç”¨é¢„è®­ç»ƒæ¨¡åž‹æ‰§è¡ŒNLPä»»åŠ¡ã€‚å®ƒå…è®¸ç”¨æˆ·è½»æ¾åœ°åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ‰§è¡Œå„ç§ä»»åŠ¡ï¼Œè€Œä¸éœ€è¦æ·±å…¥äº†è§£åº•å±‚æ¨¡åž‹ã€‚\n",
    "\n",
    "æ€»çš„æ¥è¯´ï¼Œè¿™æ®µä»£ç è®¾ç½®äº†ä½¿ç”¨Transformersåº“ä¸­é¢„è®­ç»ƒNLPæ¨¡åž‹æ‰§è¡Œå„ç§ä»»åŠ¡æ‰€éœ€çš„çŽ¯å¢ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942070c-2165-49e0-809d-61e2ee1f6b74",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "Start by creating an instance of pipeline() and specifying a task you want to use it for. In this guide, youâ€™ll use the pipeline() for **sentiment analysis** as an example:\n",
    "\n",
    "é¦–å…ˆåˆ›å»ºä¸€ä¸ªpipeline()å®žä¾‹ï¼Œå¹¶æŒ‡å®šæƒ³è¦ç”¨å®ƒæ¥æ‰§è¡Œçš„ä»»åŠ¡ã€‚åœ¨è¿™ä¸ªæŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä»¥æƒ…æ„Ÿåˆ†æžçš„pipeline()ä¸ºä¾‹è¿›è¡Œä»‹ç»ã€‚\n",
    "\n",
    "\n",
    "è¿™å¥è¯æ˜¯åœ¨ä»‹ç»å¦‚ä½•ä½¿ç”¨pipeline()æ¨¡å—æ¥æ‰§è¡ŒNLPä»»åŠ¡ã€‚å®ƒå»ºè®®æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªpipeline()å®žä¾‹ï¼Œå¹¶æŒ‡å®šè¦æ‰§è¡Œçš„ä»»åŠ¡ï¼Œä¾‹å¦‚æƒ…æ„Ÿåˆ†æžã€‚è¿™ä¸ªå®žä¾‹ä¼šè‡ªåŠ¨åŠ è½½ç›¸åº”çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼Œå¹¶å¯ä»¥ç”¨äºŽå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†æžç­‰ä»»åŠ¡ã€‚ä½¿ç”¨pipeline()å¯ä»¥ç®€åŒ–NLPä»»åŠ¡çš„æ‰§è¡Œè¿‡ç¨‹ï¼Œé¿å…äº†æ‰‹åŠ¨ä¸‹è½½å’ŒåŠ è½½æ¨¡åž‹çš„ç¹çæ­¥éª¤ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1e2049-42ef-42b1-9a0c-4b7158848630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\") # è¿™æ®µä»£ç åˆ›å»ºäº†ä¸€ä¸ªåä¸ºclassifierçš„å˜é‡ï¼Œå¹¶å°†å…¶èµ‹å€¼ä¸ºpipeline(\"sentiment-analysis\")ã€‚\n",
    "# è¿™è¡Œä»£ç ä½¿ç”¨äº†pipelineæ¨¡å—æä¾›çš„APIæ¥åˆ›å»ºä¸€ä¸ªæƒ…æ„Ÿåˆ†æžçš„pipelineå®žä¾‹ã€‚\n",
    "# å…·ä½“æ¥è¯´ï¼Œpipeline(\"sentiment-analysis\")ä¼šè‡ªåŠ¨ä¸‹è½½ç›¸åº”çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼Œå¹¶å°†å…¶åŠ è½½ä¸ºä¸€ä¸ªæƒ…æ„Ÿåˆ†æžå™¨ã€‚\n",
    "# è¿™ä¸ªæƒ…æ„Ÿåˆ†æžå™¨å¯ä»¥æŽ¥å—ä¸€æ®µæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªè¡¨ç¤ºè¯¥æ–‡æœ¬æƒ…æ„Ÿå€¾å‘çš„åˆ†æ•°ï¼Œä¾‹å¦‚æ­£é¢æƒ…æ„Ÿæˆ–è´Ÿé¢æƒ…æ„Ÿã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c16bd",
   "metadata": {},
   "source": [
    "å½“è°ƒç”¨pipeline()å‡½æ•°æ—¶æ²¡æœ‰æ˜Žç¡®æŒ‡å®šæ¨¡åž‹åç§°å’Œç‰ˆæœ¬æ—¶ï¼Œå®ƒä¼šé»˜è®¤ä½¿ç”¨ä¸€ä¸ªé¢„å…ˆæŒ‡å®šçš„æ¨¡åž‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‡ºçŽ°äº†è­¦å‘Šæ¶ˆæ¯ï¼š\"No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english). â†—.)\"ï¼Œå³é»˜è®¤ä½¿ç”¨distilbert-base-uncased-finetuned-sst-2-englishè¿™ä¸ªé¢„è®­ç»ƒæ¨¡åž‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b39c1-5460-4a18-8b77-29913fd7f7cd",
   "metadata": {},
   "source": [
    "The pipeline() downloads and caches a default pretrained model and tokenizer for sentiment analysis. Now you can use the classifier on your target text:\n",
    "\n",
    "pipeline()å‡½æ•°ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶ç¼“å­˜ä¸€ä¸ªé»˜è®¤çš„é¢„è®­ç»ƒæ¨¡åž‹å’Œåˆ†è¯å™¨ï¼Œç”¨äºŽæƒ…æ„Ÿåˆ†æžä»»åŠ¡ã€‚çŽ°åœ¨ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªåˆ†ç±»å™¨å¯¹ä½ çš„ç›®æ ‡æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†æžã€‚\n",
    "\n",
    "pipeline()å‡½æ•°ä¸‹è½½å’Œç¼“å­˜äº†æ¨¡åž‹å’Œåˆ†è¯å™¨ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªåˆ†ç±»å™¨å¯¹ä½ çš„ç›®æ ‡æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†æžï¼Œè€Œä¸éœ€è¦æ‰‹åŠ¨ä¸‹è½½å’ŒåŠ è½½æ¨¡åž‹ï¼Œä»Žè€Œç®€åŒ–äº†NLPä»»åŠ¡çš„æ‰§è¡Œè¿‡ç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150dc95a-d7fc-4c7d-a030-c2bc538bbd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = 'We are very happy to show you the ðŸ¤— Transformers library.'\n",
    "classifier(input)\n",
    "#è¿™è¡Œä»£ç å°†è¾“å…¥æ–‡æœ¬å­—ç¬¦ä¸²\" We are very happy to show you the ðŸ¤— Transformers library.\" èµ‹å€¼ç»™å˜é‡input\n",
    "#ç„¶åŽå°†è¿™ä¸ªæ–‡æœ¬è¾“å…¥åˆ°åä¸ºclassifierçš„æƒ…æ„Ÿåˆ†æžåˆ†ç±»å™¨ä¸­è¿›è¡Œæƒ…æ„Ÿåˆ†æžã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf56637",
   "metadata": {},
   "source": [
    "classifier(input)å°†è¾“å…¥æ–‡æœ¬ä¼ é€’ç»™é¢„è®­ç»ƒçš„æƒ…æ„Ÿåˆ†æžæ¨¡åž‹è¿›è¡Œå¤„ç†ï¼Œå¹¶è¿”å›žä¸€ä¸ªåŒ…å«é¢„æµ‹æƒ…æ„Ÿç±»åˆ«åŠå…¶ç½®ä¿¡åº¦å¾—åˆ†çš„Pythonå­—å…¸ã€‚\n",
    "\n",
    "è¿™ä¸ªç»“æžœå‘Šè¯‰æˆ‘ä»¬ï¼Œæ¨¡åž‹å°†è¾“å…¥æ–‡æœ¬é¢„æµ‹ä¸º\"ç§¯æž\"æƒ…æ„Ÿï¼Œå¹¶ä¸”é¢„æµ‹å¾—åˆ†éžå¸¸é«˜ [{'label': 'POSITIVE', 'score': 0.9997795224189758}]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b356db-d2c0-47c7-8249-581aa79d874c",
   "metadata": {},
   "source": [
    "If you have more than one input, pass your inputs as a list to the pipeline() to return a list of dictionaries:\n",
    "\n",
    "å¦‚æžœä½ æœ‰å¤šä¸ªè¾“å…¥ï¼Œå¯ä»¥å°†è¿™äº›è¾“å…¥ä½œä¸ºåˆ—è¡¨ä¼ é€’ç»™pipeline()å‡½æ•°ï¼Œä»¥è¿”å›žä¸€ä¸ªåŒ…å«å¤šä¸ªå­—å…¸çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå­—å…¸å¯¹åº”ä¸€ä¸ªè¾“å…¥å¹¶åŒ…å«å…¶æƒ…æ„Ÿåˆ†æžçš„ç»“æžœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7de29a-3e6b-4265-b6c8-5de71857319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "input = [\n",
    "    \"We are very happy to show you the ðŸ¤— Transformers library.\", \n",
    "    \"We hope you don't hate it.\"]\n",
    "results = classifier(input)\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1151b2-2fc1-46f5-886f-1224ecbd1e40",
   "metadata": {},
   "source": [
    "## Use another model and tokenizer in the pipeline\n",
    "The pipeline() can accommodate any model from the Hub, making it easy to adapt the pipeline() for other use-cases. For example, if youâ€™d like a model capable of handling French text, use the tags on the Hub to filter for an appropriate model. The top filtered result returns a multilingual BERT model finetuned for sentiment analysis you can use for French text.\n",
    "\n",
    "Below we use AutoModelForSequenceClassification and AutoTokenizer to load the pretrained model and itâ€™s associated tokenizer (more on an AutoClass in the next section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd0a8ca8-80d8-4d70-8b1d-2c3316614aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2b32b-bb97-4590-8c2d-dad870fd442a",
   "metadata": {},
   "source": [
    "Specify the model and tokenizer in the pipeline(), and now you can apply the classifier on French text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7aaa9e5-ff04-4eeb-a2b3-0f0a08f7d8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7272652387619019}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = 'Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.'\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fffe06-249a-4945-a866-e70bf4772736",
   "metadata": {},
   "source": [
    "If you canâ€™t find a model for your use-case, youâ€™ll need to finetune a pretrained model on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae028921-a330-4c82-ba0a-17fbbb047098",
   "metadata": {},
   "source": [
    "# AutoClass\n",
    "An AutoClass is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path.\n",
    "\n",
    "## AutoTokenizer\n",
    "A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure youâ€™re using the same tokenization rules a model was pretrained with.\n",
    "\n",
    "Load a tokenizer with AutoTokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0e43ee-0988-4955-b1ce-651b973cd9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88846f85-670c-4ed1-9209-2bb5fdcadae4",
   "metadata": {},
   "source": [
    "Pass your text to the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998012be-0bf2-4542-a145-036c6b67a09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = 'We are very happy to show you the ðŸ¤— Transformers library.'\n",
    "tokenizer(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f779e0e-19d5-4ffd-96f9-492785e63aeb",
   "metadata": {},
   "source": [
    "The tokenizer returns a dictionary containing:\n",
    "\n",
    "* **input_ids:** numerical representations of your tokens.\n",
    "* **attention_mask:** indicates which tokens should be attended to.\n",
    "\n",
    "A tokenizer can also accept a list of inputs, and **pad** and **truncate** the text to **return a batch with uniform length**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "008a2d79-40fc-4337-9da6-51d96984d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    \"We are very happy to show you the ðŸ¤— Transformers library.\", \n",
    "    \"We hope you don't hate it.\"]\n",
    "pt_batch = tokenizer(\n",
    "    raw_input,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e8ca29-66ef-49f6-b70b-5705c51d5ad0",
   "metadata": {},
   "source": [
    "## AutoModel\n",
    "HuggingFace Transformers provides a simple and unified way to load pretrained instances. This means you can load an AutoModel like you would load an AutoTokenizer. The only difference is selecting the correct AutoModel for the task. For text (or sequence) classification, you should load AutoModelForSequenceClassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c149c4-50f8-409c-a83a-7dc4f58d6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d1d85-f094-4413-ac83-af8d5155e9bd",
   "metadata": {},
   "source": [
    "> See the [task summary](https://huggingface.co/docs/transformers/task_summary) for tasks supported by an AutoModel class.\n",
    "\n",
    "Now pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding **:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e7b3ed-3e09-4376-8e94-3c2fb09d706d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6222, -2.7745, -0.8967,  2.0137,  3.3064],\n",
       "        [ 0.0064, -0.1258, -0.0503, -0.1655,  0.1329]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_outputs = pt_model(**pt_batch)\n",
    "pt_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb52f0aa-7fc3-4028-b697-c4d62cfeb7f4",
   "metadata": {},
   "source": [
    "The model outputs the final activations in the logits attribute. Note that all Transformers models output the tensors before the final activation function (like softmax) because the final activation function is often fused with the loss.\n",
    "\n",
    "Then, we apply the softmax function to the logits to retrieve the probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12a534c6-8c9f-480d-b660-e466e03e6eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n",
       "        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
    "pt_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3c208-b7ae-4fc2-94c4-c7c716783a1f",
   "metadata": {},
   "source": [
    "## Save a model\n",
    "Once your model is fine-tuned, you can save it with its tokenizer using PreTrainedModel.save_pretrained():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52e9a8a5-5dbf-47cc-9f2a-122c228d82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_save_directory = \"./pt_save_pretrained\"\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "pt_model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac521cef-fbda-47e2-b9a9-d90809a8fc23",
   "metadata": {},
   "source": [
    "When you are ready to use the model again, reload it with PreTrainedModel.from_pretrained():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5839178b-adad-4f32-a5d1-b1389a78cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90879b1-7491-42fe-b191-222f6f50b3c8",
   "metadata": {},
   "source": [
    "# Custom model builds\n",
    "You can modify the modelâ€™s configuration class to change how a model is built. The configuration specifies a modelâ€™s attributes, such as the number of hidden layers or attention heads.\n",
    "\n",
    "You start from scratch when you initialize a model from a custom configuration class. The model attributes are randomly initialized, and youâ€™ll need to train the model before you can use it to get meaningful results.\n",
    "\n",
    "Start by importing AutoConfig, and then load the pretrained model you want to modify. Within AutoConfig.from_pretrained(), you can specify the attribute you want to change, such as the number of attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "212153d4-0801-4e80-ad4b-1efda2dfaa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.30.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "my_config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", n_heads=12)\n",
    "my_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb9c0c4-bb2b-40d8-a7fa-ec0406b4ff76",
   "metadata": {},
   "source": [
    "Create a model from your custom configuration with AutoModel.from_config()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6bc1969-097a-4401-ba2c-15fe3ce54c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "custom_model = AutoModel.from_config(my_config)\n",
    "custom_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02763fa-577f-4fae-93c8-872f8a3c7b7d",
   "metadata": {},
   "source": [
    "Take a look at the [create a custom architecture guide](https://huggingface.co/docs/transformers/create_a_model) for more information about building custom configurations.\n",
    "\n",
    "# Trainer: A PyTorch optimized training loop\n",
    "All models are a standard torch.nn.Module so you can use them in any typical training loop. While you can write your own training loop, HuggingFace Transformers provides a Trainer class for PyTorch, which contains the basic training loop and adds additional functionality for features like **distributed training**, mixed precision, and more.\n",
    "\n",
    "Depending on your task, youâ€™ll typically pass the following parameters to Trainer:\n",
    "\n",
    "1. A PreTrainedModel or a torch.nn.Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a8c1bdd-136c-407f-9bb9-6cb923ff99f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc1a99-7546-430c-bdab-18a1314d09eb",
   "metadata": {},
   "source": [
    "2. TrainingArguments contains the **model hyperparameters** you can change like learning rate, batch size, and the number of epochs to train for. The default values are used if you donâ€™t specify any training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "226a6511-5402-459a-82e5-64f2284083a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=2,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_backend=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=no,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=2e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./train_args/runs/Jun22_16-29-18_ld-System-Product-Name,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=2,\n",
       "optim=adamw_hf,\n",
       "optim_args=None,\n",
       "output_dir=./train_args,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./train_args,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=500,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./train_args\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217416c-f247-466b-a173-6595f41ed235",
   "metadata": {},
   "source": [
    "3. A preprocessing class like a tokenizer, image processor, feature extractor, or processor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd644ac8-6f4a-4082-8374-4090250e1f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001de7f-fa65-42d6-af35-f337fcd6481c",
   "metadata": {},
   "source": [
    "4. Load a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6643942-b57b-4665-9214-1d49ae1cb617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset rotten_tomatoes (/home/zonghang/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692a92e906a441e292e9326feb453f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b87be-8cce-42c9-b583-b25efa402980",
   "metadata": {},
   "source": [
    "5. Create a function to tokenize the dataset, then apply it over the entire dataset with map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8204621f-d631-4743-a46b-78f50dbc9eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda60548f2374ee1b17335270c83362c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884818e360af4a60b50856e0139ce8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa73a87a69f40eaa9302b39e86e90bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    return tokenizer(dataset[\"text\"])\n",
    "\n",
    "dataset = dataset.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b42f1-b0b7-4128-8a83-2ba7420b8126",
   "metadata": {},
   "source": [
    "6. A DataCollatorWithPadding to create a batch of examples from your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "239cd4cf-2379-45c3-8c9e-47ce5df12d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f77ae-2ab6-4565-be24-1d6c5f8742fb",
   "metadata": {},
   "source": [
    "Now gather all these classes in Trainer. When youâ€™re ready, call train() to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65ea8618-565a-4486-bd03-59e14e9a402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/gpt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/zonghang/.conda/envs/gpt/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1068' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1068/1068 01:03, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.254200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/gpt/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/zonghang/.conda/envs/gpt/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1068, training_loss=0.3289355758424109, metrics={'train_runtime': 65.3118, 'train_samples_per_second': 261.208, 'train_steps_per_second': 16.352, 'total_flos': 214898859625128.0, 'train_loss': 0.3289355758424109, 'epoch': 2.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b180c-a1f1-4e22-b4b0-2bb0379b0d73",
   "metadata": {},
   "source": [
    "> For tasks - like translation or summarization - that use a sequence-to-sequence model, use the Seq2SeqTrainer and Seq2SeqTrainingArguments classes instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
