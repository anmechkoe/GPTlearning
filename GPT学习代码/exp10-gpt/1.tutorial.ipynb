{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d3ea969-cee8-4bcb-bb65-528f47be5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ad91e4",
   "metadata": {},
   "source": [
    "这段代码导入了PyTorch库和Transformers库中的pipeline模块。Transformers库是一个流行的用于自然语言处理（NLP）任务的库，例如文本分类、命名实体识别和问答等。\n",
    "\n",
    "pipeline模块提供了一个简单的API，用于使用预训练模型执行NLP任务。它允许用户轻松地加载预训练模型，并使用它们执行各种任务，而不需要深入了解底层模型。\n",
    "\n",
    "总的来说，这段代码设置了使用Transformers库中预训练NLP模型执行各种任务所需的环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942070c-2165-49e0-809d-61e2ee1f6b74",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "Start by creating an instance of pipeline() and specifying a task you want to use it for. In this guide, you’ll use the pipeline() for **sentiment analysis** as an example:\n",
    "\n",
    "首先创建一个pipeline()实例，并指定想要用它来执行的任务。在这个指南中，我们以情感分析的pipeline()为例进行介绍。\n",
    "\n",
    "\n",
    "这句话是在介绍如何使用pipeline()模块来执行NLP任务。它建议我们首先创建一个pipeline()实例，并指定要执行的任务，例如情感分析。这个实例会自动加载相应的预训练模型，并可以用于对输入文本进行情感分析等任务。使用pipeline()可以简化NLP任务的执行过程，避免了手动下载和加载模型的繁琐步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1e2049-42ef-42b1-9a0c-4b7158848630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\") # 这段代码创建了一个名为classifier的变量，并将其赋值为pipeline(\"sentiment-analysis\")。\n",
    "# 这行代码使用了pipeline模块提供的API来创建一个情感分析的pipeline实例。\n",
    "# 具体来说，pipeline(\"sentiment-analysis\")会自动下载相应的预训练模型，并将其加载为一个情感分析器。\n",
    "# 这个情感分析器可以接受一段文本作为输入，并输出一个表示该文本情感倾向的分数，例如正面情感或负面情感。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c16bd",
   "metadata": {},
   "source": [
    "当调用pipeline()函数时没有明确指定模型名称和版本时，它会默认使用一个预先指定的模型。在这种情况下，出现了警告消息：\"No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english). ↗.)\"，即默认使用distilbert-base-uncased-finetuned-sst-2-english这个预训练模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b39c1-5460-4a18-8b77-29913fd7f7cd",
   "metadata": {},
   "source": [
    "The pipeline() downloads and caches a default pretrained model and tokenizer for sentiment analysis. Now you can use the classifier on your target text:\n",
    "\n",
    "pipeline()函数会自动下载并缓存一个默认的预训练模型和分词器，用于情感分析任务。现在，你可以使用这个分类器对你的目标文本进行情感分析。\n",
    "\n",
    "pipeline()函数下载和缓存了模型和分词器，你可以使用这个分类器对你的目标文本进行情感分析，而不需要手动下载和加载模型，从而简化了NLP任务的执行过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150dc95a-d7fc-4c7d-a030-c2bc538bbd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = 'We are very happy to show you the 🤗 Transformers library.'\n",
    "classifier(input)\n",
    "#这行代码将输入文本字符串\" We are very happy to show you the 🤗 Transformers library.\" 赋值给变量input\n",
    "#然后将这个文本输入到名为classifier的情感分析分类器中进行情感分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf56637",
   "metadata": {},
   "source": [
    "classifier(input)将输入文本传递给预训练的情感分析模型进行处理，并返回一个包含预测情感类别及其置信度得分的Python字典。\n",
    "\n",
    "这个结果告诉我们，模型将输入文本预测为\"积极\"情感，并且预测得分非常高 [{'label': 'POSITIVE', 'score': 0.9997795224189758}]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b356db-d2c0-47c7-8249-581aa79d874c",
   "metadata": {},
   "source": [
    "If you have more than one input, pass your inputs as a list to the pipeline() to return a list of dictionaries:\n",
    "\n",
    "如果你有多个输入，可以将这些输入作为列表传递给pipeline()函数，以返回一个包含多个字典的列表，其中每个字典对应一个输入并包含其情感分析的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7de29a-3e6b-4265-b6c8-5de71857319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "input = [\n",
    "    \"We are very happy to show you the 🤗 Transformers library.\", \n",
    "    \"We hope you don't hate it.\"]\n",
    "results = classifier(input)\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1151b2-2fc1-46f5-886f-1224ecbd1e40",
   "metadata": {},
   "source": [
    "## Use another model and tokenizer in the pipeline\n",
    "The pipeline() can accommodate any model from the Hub, making it easy to adapt the pipeline() for other use-cases. For example, if you’d like a model capable of handling French text, use the tags on the Hub to filter for an appropriate model. The top filtered result returns a multilingual BERT model finetuned for sentiment analysis you can use for French text.\n",
    "\n",
    "Below we use AutoModelForSequenceClassification and AutoTokenizer to load the pretrained model and it’s associated tokenizer (more on an AutoClass in the next section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd0a8ca8-80d8-4d70-8b1d-2c3316614aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2b32b-bb97-4590-8c2d-dad870fd442a",
   "metadata": {},
   "source": [
    "Specify the model and tokenizer in the pipeline(), and now you can apply the classifier on French text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7aaa9e5-ff04-4eeb-a2b3-0f0a08f7d8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7272652387619019}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = 'Nous sommes très heureux de vous présenter la bibliothèque 🤗 Transformers.'\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fffe06-249a-4945-a866-e70bf4772736",
   "metadata": {},
   "source": [
    "If you can’t find a model for your use-case, you’ll need to finetune a pretrained model on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae028921-a330-4c82-ba0a-17fbbb047098",
   "metadata": {},
   "source": [
    "# AutoClass\n",
    "An AutoClass is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path.\n",
    "\n",
    "## AutoTokenizer\n",
    "A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure you’re using the same tokenization rules a model was pretrained with.\n",
    "\n",
    "Load a tokenizer with AutoTokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0e43ee-0988-4955-b1ce-651b973cd9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88846f85-670c-4ed1-9209-2bb5fdcadae4",
   "metadata": {},
   "source": [
    "Pass your text to the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998012be-0bf2-4542-a145-036c6b67a09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = 'We are very happy to show you the 🤗 Transformers library.'\n",
    "tokenizer(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f779e0e-19d5-4ffd-96f9-492785e63aeb",
   "metadata": {},
   "source": [
    "The tokenizer returns a dictionary containing:\n",
    "\n",
    "* **input_ids:** numerical representations of your tokens.\n",
    "* **attention_mask:** indicates which tokens should be attended to.\n",
    "\n",
    "A tokenizer can also accept a list of inputs, and **pad** and **truncate** the text to **return a batch with uniform length**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "008a2d79-40fc-4337-9da6-51d96984d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input = [\n",
    "    \"We are very happy to show you the 🤗 Transformers library.\", \n",
    "    \"We hope you don't hate it.\"]\n",
    "pt_batch = tokenizer(\n",
    "    raw_input,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e8ca29-66ef-49f6-b70b-5705c51d5ad0",
   "metadata": {},
   "source": [
    "## AutoModel\n",
    "HuggingFace Transformers provides a simple and unified way to load pretrained instances. This means you can load an AutoModel like you would load an AutoTokenizer. The only difference is selecting the correct AutoModel for the task. For text (or sequence) classification, you should load AutoModelForSequenceClassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c149c4-50f8-409c-a83a-7dc4f58d6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d1d85-f094-4413-ac83-af8d5155e9bd",
   "metadata": {},
   "source": [
    "> See the [task summary](https://huggingface.co/docs/transformers/task_summary) for tasks supported by an AutoModel class.\n",
    "\n",
    "Now pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding **:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e7b3ed-3e09-4376-8e94-3c2fb09d706d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6222, -2.7745, -0.8967,  2.0137,  3.3064],\n",
       "        [ 0.0064, -0.1258, -0.0503, -0.1655,  0.1329]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_outputs = pt_model(**pt_batch)\n",
    "pt_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb52f0aa-7fc3-4028-b697-c4d62cfeb7f4",
   "metadata": {},
   "source": [
    "The model outputs the final activations in the logits attribute. Note that all Transformers models output the tensors before the final activation function (like softmax) because the final activation function is often fused with the loss.\n",
    "\n",
    "Then, we apply the softmax function to the logits to retrieve the probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12a534c6-8c9f-480d-b660-e466e03e6eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n",
       "        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
    "pt_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3c208-b7ae-4fc2-94c4-c7c716783a1f",
   "metadata": {},
   "source": [
    "## Save a model\n",
    "Once your model is fine-tuned, you can save it with its tokenizer using PreTrainedModel.save_pretrained():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52e9a8a5-5dbf-47cc-9f2a-122c228d82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_save_directory = \"./pt_save_pretrained\"\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "pt_model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac521cef-fbda-47e2-b9a9-d90809a8fc23",
   "metadata": {},
   "source": [
    "When you are ready to use the model again, reload it with PreTrainedModel.from_pretrained():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5839178b-adad-4f32-a5d1-b1389a78cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90879b1-7491-42fe-b191-222f6f50b3c8",
   "metadata": {},
   "source": [
    "# Custom model builds\n",
    "You can modify the model’s configuration class to change how a model is built. The configuration specifies a model’s attributes, such as the number of hidden layers or attention heads.\n",
    "\n",
    "You start from scratch when you initialize a model from a custom configuration class. The model attributes are randomly initialized, and you’ll need to train the model before you can use it to get meaningful results.\n",
    "\n",
    "Start by importing AutoConfig, and then load the pretrained model you want to modify. Within AutoConfig.from_pretrained(), you can specify the attribute you want to change, such as the number of attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "212153d4-0801-4e80-ad4b-1efda2dfaa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.30.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "my_config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", n_heads=12)\n",
    "my_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb9c0c4-bb2b-40d8-a7fa-ec0406b4ff76",
   "metadata": {},
   "source": [
    "Create a model from your custom configuration with AutoModel.from_config()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6bc1969-097a-4401-ba2c-15fe3ce54c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "custom_model = AutoModel.from_config(my_config)\n",
    "custom_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02763fa-577f-4fae-93c8-872f8a3c7b7d",
   "metadata": {},
   "source": [
    "Take a look at the [create a custom architecture guide](https://huggingface.co/docs/transformers/create_a_model) for more information about building custom configurations.\n",
    "\n",
    "# Trainer: A PyTorch optimized training loop\n",
    "All models are a standard torch.nn.Module so you can use them in any typical training loop. While you can write your own training loop, HuggingFace Transformers provides a Trainer class for PyTorch, which contains the basic training loop and adds additional functionality for features like **distributed training**, mixed precision, and more.\n",
    "\n",
    "Depending on your task, you’ll typically pass the following parameters to Trainer:\n",
    "\n",
    "1. A PreTrainedModel or a torch.nn.Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a8c1bdd-136c-407f-9bb9-6cb923ff99f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc1a99-7546-430c-bdab-18a1314d09eb",
   "metadata": {},
   "source": [
    "2. TrainingArguments contains the **model hyperparameters** you can change like learning rate, batch size, and the number of epochs to train for. The default values are used if you don’t specify any training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "226a6511-5402-459a-82e5-64f2284083a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=2,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_backend=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=no,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=2e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./train_args/runs/Jun22_16-29-18_ld-System-Product-Name,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=2,\n",
       "optim=adamw_hf,\n",
       "optim_args=None,\n",
       "output_dir=./train_args,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./train_args,\n",
       "save_on_each_node=False,\n",
       "save_safetensors=False,\n",
       "save_steps=500,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./train_args\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217416c-f247-466b-a173-6595f41ed235",
   "metadata": {},
   "source": [
    "3. A preprocessing class like a tokenizer, image processor, feature extractor, or processor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd644ac8-6f4a-4082-8374-4090250e1f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001de7f-fa65-42d6-af35-f337fcd6481c",
   "metadata": {},
   "source": [
    "4. Load a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6643942-b57b-4665-9214-1d49ae1cb617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset rotten_tomatoes (/home/zonghang/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692a92e906a441e292e9326feb453f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b87be-8cce-42c9-b583-b25efa402980",
   "metadata": {},
   "source": [
    "5. Create a function to tokenize the dataset, then apply it over the entire dataset with map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8204621f-d631-4743-a46b-78f50dbc9eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda60548f2374ee1b17335270c83362c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884818e360af4a60b50856e0139ce8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa73a87a69f40eaa9302b39e86e90bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    return tokenizer(dataset[\"text\"])\n",
    "\n",
    "dataset = dataset.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b42f1-b0b7-4128-8a83-2ba7420b8126",
   "metadata": {},
   "source": [
    "6. A DataCollatorWithPadding to create a batch of examples from your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "239cd4cf-2379-45c3-8c9e-47ce5df12d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f77ae-2ab6-4565-be24-1d6c5f8742fb",
   "metadata": {},
   "source": [
    "Now gather all these classes in Trainer. When you’re ready, call train() to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65ea8618-565a-4486-bd03-59e14e9a402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/gpt/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/zonghang/.conda/envs/gpt/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1068' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1068/1068 01:03, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.254200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/gpt/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/zonghang/.conda/envs/gpt/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1068, training_loss=0.3289355758424109, metrics={'train_runtime': 65.3118, 'train_samples_per_second': 261.208, 'train_steps_per_second': 16.352, 'total_flos': 214898859625128.0, 'train_loss': 0.3289355758424109, 'epoch': 2.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b180c-a1f1-4e22-b4b0-2bb0379b0d73",
   "metadata": {},
   "source": [
    "> For tasks - like translation or summarization - that use a sequence-to-sequence model, use the Seq2SeqTrainer and Seq2SeqTrainingArguments classes instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
