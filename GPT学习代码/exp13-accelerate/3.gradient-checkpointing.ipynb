{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6d34fe-9cc5-404f-b01b-5000039f0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077c6d79-bf34-4f1d-88b5-0885d3339c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 620 MB.\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    # For GPU 0\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    memory_used = info.used\n",
    "    # For GPU 1\n",
    "    handle = nvmlDeviceGetHandleByIndex(1)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    memory_used += info.used\n",
    "    print(f\"GPU memory occupied: {memory_used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e3e321-33e3-48da-8811-934ddadd5d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/accelerate/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 2564 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\").to('cuda:0')\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a6bf17-8691-43cc-945a-11436b23d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    \"output_dir\": \"outputs\",\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0c742c-752e-4944-9d28-307efcc0a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 1, (dataset_size)),\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09889b6e-e50c-41f4-b6f5-1fddbe2e420b",
   "metadata": {},
   "source": [
    "# Gradient Checkpointing\n",
    "Even when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models.\n",
    "\n",
    "In order to compute the gradients during the backward pass, **all activations from the forward pass are normally saved**. This can create a big memory overhead.\n",
    "\n",
    "Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training.\n",
    "\n",
    "**Gradient checkpointing** strikes a compromise between the two approaches and **saves selected activations** throughout the computational graph, so **only a fraction of the activations need to be re-computed for the gradients**. See this [article](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9) explaining the ideas behind gradient checkpointing.\n",
    "\n",
    "To enable gradient checkpointing in the Trainer we only need to pass it as a flag to the TrainingArguments. Everything else is handled under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80581a61-55da-4b84-b80e-8c4bf76ebdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/accelerate/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 01:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 63.28\n",
      "Samples/second: 8.09\n",
      "GPU memory occupied: 7078 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1, \n",
    "    gradient_accumulation_steps=4, \n",
    "    gradient_checkpointing=True, \n",
    "    **default_args\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=ds\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97288af-9f55-44d3-9cc1-5f1467c4a241",
   "metadata": {},
   "source": [
    "Here we compare the results with the one enabing gradient accumulation but disabling gradient checkpointing:\n",
    "\n",
    "* Time: 48.18 (Increase to 63.28 seconds)\n",
    "* Samples/second: 10.63 (Decrease to 8.09 samples per second)\n",
    "* GPU memory occupied: 8590 MB **(Decrease to 7078 MB)**\n",
    "\n",
    "We can see that this **saved some more memory** but at the same time **training became a bit slower**. A general rule of thumb is that **gradient checkpointing slows down training by about 20%**.\n",
    "\n",
    "Then, let's train with a larger batch size of 64 to make better use of the available GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b621a1-08d0-4018-a7ef-e5c6878ca752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/accelerate/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 46.53\n",
      "Samples/second: 11.00\n",
      "GPU memory occupied: 8030 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8, \n",
    "    gradient_accumulation_steps=8, \n",
    "    gradient_checkpointing=True, \n",
    "    **default_args\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=ds\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21672b7-e181-4189-92a2-22d7e7821f5f",
   "metadata": {},
   "source": [
    "Here we compare the results with the baseline metrics (with a batch size of 64 and gradient accumulation enabled):\n",
    "\n",
    "* Time: 34.26 (Increase to 46.53 seconds)\n",
    "* Samples/second: 14.94 (Decrease to 11 samples per second)\n",
    "* GPU memory occupied: 20314 MB **(Decrease to 8030 MB)**\n",
    "\n",
    "Finally, let's try **how large a batch size can be applied to maximum the memory usage** of an NVIDIA Geforce RTX 3090 (with a total of 24 GB of CUDA memory). The answer is: **120**!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67474089-a924-42e4-b1b9-9e2efd7a875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/accelerate/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43.78\n",
      "Samples/second: 11.70\n",
      "GPU memory occupied: 24600 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=120, \n",
    "    gradient_accumulation_steps=1, \n",
    "    gradient_checkpointing=True, \n",
    "    **default_args\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=ds\n",
    ")\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "print_summary(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
