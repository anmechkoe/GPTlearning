{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6d34fe-9cc5-404f-b01b-5000039f0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077c6d79-bf34-4f1d-88b5-0885d3339c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 620 MB.\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    # For GPU 0\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    memory_used = info.used\n",
    "    # For GPU 1\n",
    "    handle = nvmlDeviceGetHandleByIndex(1)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    memory_used += info.used\n",
    "    print(f\"GPU memory occupied: {memory_used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e3e321-33e3-48da-8811-934ddadd5d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zonghang/.conda/envs/accelerate/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 2564 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\").to('cuda:0')\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a6bf17-8691-43cc-945a-11436b23d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    \"output_dir\": \"outputs\",\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0c742c-752e-4944-9d28-307efcc0a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 1, (dataset_size)),\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09889b6e-e50c-41f4-b6f5-1fddbe2e420b",
   "metadata": {},
   "source": [
    "# Using Hugging Face Accelerate\n",
    "So far we have used the Trainer to run the experiments but a more flexible alternative to that approach is to use Hugging Face Accelerate. With Accelerate, you have full control over the training loop and can essentially write the loop in pure PyTorch with some minor modifications. In turn it allows you to **easily scale across different infrastructures such as CPUs, GPUs, TPUs, or distributed multi-GPU setups** without changing any code.\n",
    "\n",
    "Letâ€™s see what it takes to implement all of the above tweaks in Accelerate. We can still use the TrainingArguments to wrap the training settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08327f7c-c7d7-40e5-81da-afdca25cfa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\",\n",
    "    **default_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf4df3-c8e9-4ec1-bca3-b85713b9f2c1",
   "metadata": {},
   "source": [
    "The full example code with Accelerate is give below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a60617-9104-445f-8075-05afd087252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45.45839309692383\n",
      "GPU memory occupied: 6432 MB.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import Adafactor\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# Wrap the dataset in a DataLoader\n",
    "dataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# Define the AdaFactor optimizer\n",
    "optim = Adafactor(model.parameters(), beta1=training_args.adam_beta1)\n",
    "\n",
    "# Specify if we want to use mixed precision training,\n",
    "# and it will take care of it for us in the prepare call.\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "model, optimizer, dataloader = accelerator.prepare(model, optim, dataloader)\n",
    "\n",
    "# Set the model in training mode\n",
    "model.train()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Main training loop\n",
    "for step, batch in enumerate(dataloader, start=1):\n",
    "    loss = model(**batch).loss\n",
    "\n",
    "    # Normalize the loss so we get the average at the end of accumulation\n",
    "    loss = loss / training_args.gradient_accumulation_steps\n",
    "\n",
    "    accelerator.backward(loss)\n",
    "    \n",
    "    if step % training_args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "print(f'Time: {time.time() - start_time}')\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73280e-2ce0-4693-9597-ae0f604aa033",
   "metadata": {},
   "source": [
    "Here we compare the results with the one implemented on pure transformers:\n",
    "\n",
    "* Time: 42.19 (Increase to 45.46 seconds)\n",
    "* GPU memory occupied: 5142 MB (Increase to 6432 MB)\n",
    "\n",
    "Implementing these optimization techniques with Accelerate only takes a handful of lines of code and comes with the benefit of **more flexiblity in the training loop**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
