{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6796d1b7-8218-4045-90be-0039a626569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "def count_params(model):\n",
    "    params = utils.parameters_to_vector(model.parameters())\n",
    "    num_params = torch.numel(params)\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4487c8-703d-4893-b887-567491a38361",
   "metadata": {},
   "source": [
    "# OpenAI GPT\n",
    "OpenAI GPT model was proposed in [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf). It’s a causal (unidirectional) transformer pre-trained language model on a large corpus will long range dependencies, the Toronto Book Corpus.\n",
    "\n",
    "[Write With Transformer](https://transformer.huggingface.co/doc/gpt) is a webapp created and hosted by Hugging Face showcasing the generative capabilities of several models. GPT is one of them. You can try its performance on the website.\n",
    "\n",
    "This model was contributed by thomwolf. The original code can be found [here](https://github.com/openai/finetune-transformer-lm).\n",
    "\n",
    "> If you want to reproduce the original tokenization process of the OpenAI GPT paper, you will need to install ftfy and SpaCy:\n",
    "> ```\n",
    "> pip install spacy ftfy==4.4.3\n",
    "> python -m spacy download en\n",
    "> ```\n",
    "> \n",
    "> If you don’t install ftfy and SpaCy, the OpenAIGPTTokenizer will default to tokenize using BERT’s BasicTokenizer followed by Byte-Pair Encoding (which should be fine for most usage, don’t worry)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837669f2-2c99-40f0-9aa7-5c529389801a",
   "metadata": {},
   "source": [
    "## OpenAIGPTModel\n",
    "This bare OpenAI GPT transformer model outputting raw hidden-states without any specific head on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54617cb2-b012-4e8f-8557-815e4d61defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, OpenAIGPTModel\n",
    "import torch\n",
    "\n",
    "# Instantiate the tokenizer using the \"openai-gpt\" pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "# Tokenize the input sentence using the tokenizer\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "# Instantiate the OpenAI GPT model using the \"openai-gpt\" pre-trained weights\n",
    "model = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "# Move the model and inputs to the GPU (assuming CUDA is available)\n",
    "model, inputs = model.to('cuda:0'), inputs.to('cuda:0')\n",
    "\n",
    "# Perform a forward pass through the model by passing the tokenized inputs\n",
    "# The model will generate the outputs based on the input tokens\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac5dba65-9aac-4a1f-a0b0-e1d8954c73ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIGPTTokenizerFast(name_or_path='openai-gpt', vocab_size=40478, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '<unk>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3196d67c-152e-4d5f-9ac2-ca9c3b3adaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f57d1c0-1cb8-4223-9861-a49f336bc3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIGPTModel(\n",
       "  (tokens_embed): Embedding(40478, 768)\n",
       "  (positions_embed): Embedding(512, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa69b936-0765-43bc-bdae-d24f076cdd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in GPT: 116534784\n"
     ]
    }
   ],
   "source": [
    "print('Number of parameters in GPT:', count_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35487666-271d-4455-af80-9fa92069c071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4653,  0.0642,  0.5910,  ...,  0.1177, -0.0021, -1.2262],\n",
       "          [-0.3697, -0.0957,  0.6613,  ..., -0.0344, -0.2164,  0.1205],\n",
       "          [ 0.1700, -0.3252,  0.0407,  ...,  0.1589, -0.8057, -0.2830],\n",
       "          [-0.3669, -0.0448,  0.8061,  ..., -0.0090, -0.0872, -0.5224],\n",
       "          [-0.5047,  0.6522,  0.6932,  ...,  0.0811,  0.6475,  0.3190],\n",
       "          [-0.2972,  0.0591,  1.2333,  ..., -0.7394, -0.2600,  0.0863]]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " torch.Size([1, 6, 768]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state, outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e301217-d684-4695-96de-8937fb710179",
   "metadata": {},
   "source": [
    "## OpenAIGPTLMHeadModel\n",
    "OpenAI GPT Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "792a62e3-dc47-432c-ae19-90955ecf2695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, OpenAIGPTLMHeadModel\n",
    "import torch\n",
    "\n",
    "# Instantiate the tokenizer using the \"openai-gpt\" pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "# Tokenize the input sentence using the tokenizer\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "# Instantiate the OpenAI GPT model using the \"openai-gpt\" pre-trained weights\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "# Move the model and inputs to the GPU (assuming CUDA is available)\n",
    "model, inputs = model.to('cuda:0'), inputs.to('cuda:0')\n",
    "\n",
    "# Perform a forward pass through the model by passing the tokenized inputs\n",
    "# The model will generate the outputs based on the input tokens\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1522894-1dac-4376-a184-7e1af37e7337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIGPTLMHeadModel(\n",
       "  (transformer): OpenAIGPTModel(\n",
       "    (tokens_embed): Embedding(40478, 768)\n",
       "    (positions_embed): Embedding(512, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=40478, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2da5b77e-b90d-481d-9de0-2b666952b274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
       "          [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
       "          [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
       "          [ -5.6463,  -5.9526, -17.5195,  ...,  -9.4144, -15.7120,  -1.5394],\n",
       "          [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
       "          [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416]]],\n",
       "        device='cuda:0', grad_fn=<UnsafeViewBackward0>),\n",
       " torch.Size([1, 6, 40478]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits, outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af712bad-19c2-4fd2-8726-fc80c50f38bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[', \" name. a.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the token ids from the outputs.logits\n",
    "token_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Decode the token ids using the tokenizer\n",
    "tokenizer.batch_decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d562473f-dd3b-4004-8c0b-46e6a2ce637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, my dog is cute.  \" \\n \" he\\'s my new favorite toy. \" \\n \" i\\'m really sorry. \" \\n \" you know'},\n",
       " {'generated_text': 'Hello, my dog is cute.  marketplace and all that. \" \\n \" thank you. \" \\n she didn\\'t miss how his eyes lit up as'},\n",
       " {'generated_text': 'Hello, my dog is cute.  is that cat with you? \" \\n \" she\\'s asleep. \" he stepped back for her and she followed him'},\n",
       " {'generated_text': 'Hello, my dog is cute.  the three of us were going to meet up there early at a place called... \" \\n \" the beach, \"'},\n",
       " {'generated_text': 'Hello, my dog is cute.  she says hi when you say hello to her. i will do it for her. okay, here she comes.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='openai-gpt')\n",
    "generator(\"Hello, my dog is cute. \", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8d822-347a-45e7-a408-c1ce45c1e97f",
   "metadata": {},
   "source": [
    "## GPT-2\n",
    "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, **it was trained to guess the next word in sentences.**\n",
    "\n",
    "Test the whole generation capabilities [here](https://transformer.huggingface.co/doc/gpt2-large).\n",
    "\n",
    "Here we use the **smallest** version of GPT-2, with 124M parameters and 548MB of storage occupation.\n",
    "\n",
    "You can use the raw model for text generation or fine-tune it to a downstream task. See the [model hub](https://huggingface.co/models?other=gpt2) to look for fine-tuned versions on a task that interests you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29171a8f-8eb8-4a93-861a-a5d1696c6f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, my dog is cute. \\xa0When I try to go back and have some fun with my little friend or friend friend, my dog becomes'},\n",
       " {'generated_text': \"Hello, my dog is cute. \\xa0He can't breathe properly and I'm trying to help him breathe. I love it because it keeps my\"},\n",
       " {'generated_text': \"Hello, my dog is cute. ㅇㅇㅇㅇㅇ I hate to know what you'd\"},\n",
       " {'generated_text': 'Hello, my dog is cute. \\xa0Do you want to go back to your garden or do you want her to show you any cute housemates'},\n",
       " {'generated_text': \"Hello, my dog is cute. \\xa0She has gone through all the surgeries and she's so loving. \\xa0She's so excited. \"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2', device='cuda:0')\n",
    "generator(\"Hello, my dog is cute.\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1151778-ef16-4cc4-8538-49eef223be5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43e97710-6cc5-4368-8570-2ffdd9c53ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in GPT-2: 124439808\n"
     ]
    }
   ],
   "source": [
    "print('Number of parameters in GPT-2:', count_params(generator.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e4e63-4386-4ab9-b880-c2ff437a0fc4",
   "metadata": {},
   "source": [
    "## GPT-2 Medium\n",
    "GPT-2 Medium is the **355M parameters (1.52GB of storage occupation.)** version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective.\n",
    "\n",
    "Use the code below to get started with the model. You can use this model directly with a pipeline for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7058a14a-445c-430c-be0f-0619a88e297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, my dog is cute. How were you? Can you tell him I like him?\"\\n\\nLydia is a very friendly pet. I'},\n",
       " {'generated_text': \"Hello, my dog is cute. He was just talking to me on the telephone and I can't wait to meet him! Hello… I don't\"},\n",
       " {'generated_text': 'Hello, my dog is cute. He is a real good dog!\\n\\n\"My dog has been an excellent trainer. When I think of his'},\n",
       " {'generated_text': 'Hello, my dog is cute. He has some kind of animal-like features. He says he can do this or that. Can you explain that'},\n",
       " {'generated_text': 'Hello, my dog is cute. But I\\'m just so uncomfortable with my dog.\" It\\'s almost like an extension of her guilt about her \"good'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2-medium', device='cuda:0')\n",
    "generator(\"Hello, my dog is cute.\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2488fc5c-2074-43ec-8929-83297e357f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91d18908-bd2b-4e4e-afe5-a93870f84cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in GPT-2 Medium: 354823168\n"
     ]
    }
   ],
   "source": [
    "print('Number of parameters in GPT-2 Medium:', count_params(generator.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d380c-a657-4f4c-ad9f-530e35d94125",
   "metadata": {},
   "source": [
    "## GPT-2 Large\n",
    "GPT-2 Large is the **774M parameters (3.25GB of storage occupation.)** version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective.\n",
    "\n",
    "Test the full generation capabilities [here](https://transformer.huggingface.co/doc/gpt2-large).\n",
    "\n",
    "Use the code below to get started with the model. You can use this model directly with a pipeline for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c941f4a-1e5f-41de-b8c9-62cdf9c556a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, my dog is cute. He just got a nose job.\\n\\nIt started a few weeks ago. It had always been possible; so'},\n",
       " {'generated_text': 'Hello, my dog is cute. She has a sweet face, she is funny, but most of all she is loyal. I would have to give'},\n",
       " {'generated_text': 'Hello, my dog is cute. He loves her! How should I look at him? What should I say to tell him how pretty he is?'},\n",
       " {'generated_text': 'Hello, my dog is cute. My dog loves the water and my family loves to go jogging.\"'},\n",
       " {'generated_text': 'Hello, my dog is cute. I just couldn\\'t resist the urge to put him in a new toy!\" I looked over at the box of T'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2-large', device='cuda:0')\n",
    "generator(\"Hello, my dog is cute.\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa48a51-06ac-49f7-8b0b-f107d0b87d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2239f7-de4b-4b1b-ae6c-5fba17a9e223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in GPT-2 Large: 774030080\n"
     ]
    }
   ],
   "source": [
    "print('Number of parameters in GPT-2 Large:', count_params(generator.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f74f6-4adf-4eb8-8d1e-9ba1d86b7792",
   "metadata": {},
   "source": [
    "## GPT-2 XL\n",
    "GPT-2 XL is the **1.5B parameters (6.43GB of storage occupation.)** version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective.\n",
    "\n",
    "Use the code below to get started with the model. You can use this model directly with a pipeline for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa2fb51-49db-48b8-aa60-42d03d6251ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, my dog is cute. How did you get her?\"\\n\\n\"What I said before isn\\'t funny. Now, get off your big'},\n",
       " {'generated_text': 'Hello, my dog is cute. Let\\'s play.\" and, \"He\\'s my favorite toy.\" \"Let\\'s play Frisbee?\" And that'},\n",
       " {'generated_text': 'Hello, my dog is cute. What happens to me?\"\\n\\n-Sharon\\n\\nWhat is a good dog food commercial?\\n\\nThe'},\n",
       " {'generated_text': 'Hello, my dog is cute. You see a picture of my dog? Oh, I like you too. So how about you meet my dog?\"'},\n",
       " {'generated_text': 'Hello, my dog is cute. I have to get her something new too! When I saw this, I was looking for a similar option for my'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2-xl', device='cuda:0')\n",
    "generator(\"Hello, my dog is cute.\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff780811-25df-41c1-99a7-9a33b832951e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b182a6-a20e-4722-a029-525c5d198c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in GPT-2 XL: 1557611200\n"
     ]
    }
   ],
   "source": [
    "print('Number of parameters in GPT-2 XL:', count_params(generator.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413bedf-ef08-4e80-92f1-bd793ff44e95",
   "metadata": {},
   "source": [
    "## DistillGPT2\n",
    "DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. DistilGPT2, which has **82 million parameters**, was developed using **knowledge distillation** and was designed to be a **faster, lighter version of GPT-2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c693489c-9494-4a8d-ac86-0be6ff878c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, my dog is cute. I love dogs, like dog. I always bring him food.'},\n",
       " {'generated_text': 'Hello, my dog is cute.\\u202a It is the only color you can see.‣'},\n",
       " {'generated_text': 'Hello, my dog is cute. I have a dog called \"Karen\" and she keeps it'},\n",
       " {'generated_text': \"Hello, my dog is cute. She's a loving and loving dog.\\n\\nIf you�\"},\n",
       " {'generated_text': \"Hello, my dog is cute. I really didn't want it to be all my own and he\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='distilgpt2', device='cuda:0')\n",
    "generator(\"Hello, my dog is cute.\", max_length=20, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ec210b-5cb3-4f2b-a0ee-68bcae6e4e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dcdb99f-a3ee-47e0-b8da-04b8312dea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in GPT-2 XL: 81912576\n"
     ]
    }
   ],
   "source": [
    "print('Number of parameters in GPT-2 XL:', count_params(generator.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557ae1b-426a-43fe-9ae8-099c8a866c47",
   "metadata": {},
   "source": [
    "## GPT-Neo 1.3B (GPT-3)\n",
    "GPT-Neo 1.3B is a transformer model designed using EleutherAI's **replication of the GPT-3 architecture**. GPT-Neo refers to the class of models, while **1.3B represents the number of parameters (5.31GB of storage occupation)** of this pre-trained model.\n",
    "\n",
    "This model was trained on the Pile for **380 billion tokens over 362,000 steps**. It was trained as a masked autoregressive language model, using cross-entropy loss.\n",
    "\n",
    "You can use this model directly with a pipeline for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697b6944-e090-4edb-a38b-4035604425ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/zonghang/.conda/envs/gpt/lib/python3.8/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'EleutherAI has introduced a brand new release with an emphasis on the usability and performance of the product. The version 2.0.10.0 is also more advanced than previous versions, including a brand new user interface, a new networking module and'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B', device='cuda:0')\n",
    "generator(\"EleutherAI has\", do_sample=True, min_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e14b541-31a5-463d-a69f-f1b79c0dde27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 2048)\n",
       "    (wpe): Embedding(2048, 2048)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49fa13c-fbbf-4c43-abcb-85f5f60df320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in GPT-Neo 1.3B: 1315575808\n"
     ]
    }
   ],
   "source": [
    "print('Number of parameters in GPT-Neo 1.3B:', count_params(generator.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390534cf-7dd5-4ba0-827a-ed54adbb2204",
   "metadata": {},
   "source": [
    "## GPT-Neo 2.7B (GPT-3)\n",
    "GPT-Neo 2.7B is a transformer model designed using EleutherAI's **replication of the GPT-3 architecture**. GPT-Neo refers to the class of models, while **2.7B represents the number of parameters (10.7GB of storage occupation)** of this pre-trained model.\n",
    "\n",
    "This model was trained for 420 billion tokens over 400,000 steps. It was trained as a masked autoregressive language model, using cross-entropy loss.\n",
    "\n",
    "You can use this model directly with a pipeline for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca97abed-6021-44cc-b1ba-633cf1b47bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/zonghang/.conda/envs/gpt/lib/python3.8/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'EleutherAI has released their new version of its open source artificial intelligence software, dubbed Eleuther AI Open Core. Rather than create a new, custom version of the software or make existing open-source components for the software, EleutherAI decided to'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B', device='cuda:0')\n",
    "generator(\"EleutherAI has\", do_sample=True, min_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e30f8d-9eab-4c85-b373-0203ac5a2b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 2560)\n",
       "    (wpe): Embedding(2048, 2560)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23723e6b-43f3-4dab-8b7d-e89d2c6c0b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in GPT-Neo 2.7B: 2651307520\n"
     ]
    }
   ],
   "source": [
    "print('Number of parameters in GPT-Neo 2.7B:', count_params(generator.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7377a08d-a40a-4a6e-8750-3bde71ac412c",
   "metadata": {},
   "source": [
    "## GPT-NeoX-20B (GPT-3)\n",
    "GPT-NeoX-20B is a **20 billion parameter (~40GB of storage occupation, tensor type: fp16)** autoregressive language model trained on the Pile using the GPT-NeoX library. **Its architecture intentionally resembles that of GPT-3, and is almost identical to that of GPT-J-6B.**\n",
    "\n",
    "GPT-NeoX-20B has not been fine-tuned for downstream tasks for which language models are commonly deployed, such as writing genre prose, or commercial chatbots. This means GPT-NeoX-20B will likely **not** respond to a given prompt the way products such as ChatGPT do. This is because, unlike GPT-NeoX-20B, ChatGPT was fine-tuned using methods such as Reinforcement Learning from Human Feedback (RLHF) to better “understand” human instructions and dialogue.\n",
    "\n",
    "This model is **English-language only**, and thus cannot be used for translation or generating text in other languages.\n",
    "\n",
    "GPT-NeoX-20B was trained with a batch size of approximately **3.15M tokens** (1538 sequences of 2048 tokens each), for a total of **150,000 steps**. **Tensor parallelism** and **pipeline parallelism** were used to distribute the model across GPUs.\n",
    "\n",
    "If you simply want to try out some prompts, check out this [playground](https://20b.eleuther.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6232ad3-1fb7-46b3-8a68-82101c2048b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9edfb54358c459591f5822e712cfe25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/zonghang/.conda/envs/gpt/lib/python3.8/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'EleutherAI has a built-in OMS.\\n\\n(2) Can only be used'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neox-20b', device_map='auto')\n",
    "generator(\"EleutherAI has\", do_sample=True, min_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75f0465-fc4c-40b6-82e5-3d90a632ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50432, 6144)\n",
       "    (layers): ModuleList(\n",
       "      (0-43): 44 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=6144, out_features=18432, bias=True)\n",
       "          (dense): Linear(in_features=6144, out_features=6144, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=6144, out_features=24576, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=24576, out_features=6144, bias=True)\n",
       "          (act): FastGELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=6144, out_features=50432, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e60720-3872-4dad-8cf6-50fb99bb0d0c",
   "metadata": {},
   "source": [
    "# LLAMA\n",
    "LLaMA is an auto-regressive language model, based on the transformer architecture. More information can be found in the paper “[LLaMA, Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)”.\n",
    "\n",
    "The primary use of LLaMA is research on large language models, including: exploring potential applications such as **question answering**, **natural language understanding** or **reading comprehension**, understanding capabilities and limitations of current language models, and developing techniques to improve those, evaluating and mitigating biases, risks, toxic and harmful content generations, hallucinations.\n",
    "\n",
    "One of the most relevant factors for which model performance may vary is which language is used. Although this model included 20 languages in the training data, most of our dataset is made of English text, and we thus expect the model to **perform better for English than other languages**.\n",
    "\n",
    "In this tutorial, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a **3B** and **7B** model trained on 1T tokens, as well as the preview of a **13B** model trained on 600B tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a06ec0-94b1-44be-9d90-0467f71c75b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "/home/zonghang/.conda/envs/gpt/lib/python3.8/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Q: What is the largest animal?\n",
      "A: The blue whale.\n",
      "Q: What is the largest animal?\n",
      "A: The blue whale. It is the largest animal on Earth. It is also the\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b'\n",
    "# model_path = 'openlm-research/open_llama_7b'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "\n",
    "prompt = 'Q: What is the largest animal?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(input_ids=input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7c3c88-0ac7-47bc-ab09-ae808b821f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 3200, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "          (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3200, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579dfd04-1570-4539-b10a-c43d7c44d1ce",
   "metadata": {},
   "source": [
    "## ChatGLM2-6B\n",
    "ChatGLM2-6B 是开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM2-6B 引入了如下新特性：\n",
    "\n",
    "1. **更强大的性能：** 基于 ChatGLM 初代模型的开发经验，全面升级了 ChatGLM2-6B 的基座模型。ChatGLM2-6B 使用了 GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，评测结果显示，相比于初代模型，ChatGLM2-6B 在 MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。\n",
    "2. **更长的上下文：** 基于 FlashAttention 技术，将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的 2K 扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练，允许更多轮次的对话。\n",
    "3. **更高效的推理：** 基于 Multi-Query Attention 技术，ChatGLM2-6B 有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4 量化下，6G 显存支持的对话长度由 1K 提升到了 8K。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e31ab2db-4335-4d9e-a6ff-63a447b54c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type chatglm to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7912c2ae0de74a03bd472bafd47d8e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\n",
      "\n",
      "ChatGLM2-6B 是基于 GLM2-6B 模型开发的，而 GLM2-6B 模型是基于 GLM 模型开发的。GLM （General Language Modeling）是一种由清华大学 KEG 实验室提出的结合了 BERT 和 GPT 优势的通用预训练模型。\n",
      "\n",
      "具体来说，ChatGLM2-6B 相比 GLM 模型的区别主要在以下几个方面：\n",
      "\n",
      "1. 训练目标：ChatGLM2-6B 的训练目标是更加关注用户对话体验，因此在对用户发起的对话回复中，更加关注用户的情感需求。\n",
      "\n",
      "2. 接口类型：ChatGLM2-6B 使用的接口类型是 B 接口，而 GLM 模型的接口类型是 A 接口。\n",
      "\n",
      "3. 能力不同：ChatGLM2-6B 模型的能力更加关注于对话的回复，特别是回复的长度和复杂程度。\n",
      "\n",
      "总的来说，ChatGLM2-6B 模型是 GLM 模型的一种特殊版本，主要关注用户对话体验和回复的长度和复杂程度。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_repo = \"THUDM/chatglm2-6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo, trust_remote_code=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_repo, torch_dtype=torch.float16, trust_remote_code=True, device='cuda:0')\n",
    "\n",
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "print(response, end='\\n\\n')\n",
    "\n",
    "response, history = model.chat(tokenizer, \"ChatGLM2-6B的GPT2的本质区别是什么？\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630623a0-adbe-4afe-86f6-eaa702b85944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是ChatGLM2-6B，一个基于语言模型的人工智能助手。我的训练数据截止到2023年。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"ChatGLM2-6B的训练数据截止到哪一年？\", history=[])\n",
    "print(response, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4137e3a4-d925-469c-a9d4-723ab620654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in ChatGLM2-6B: 6243584000\n"
     ]
    }
   ],
   "source": [
    "print('Number of parameters in ChatGLM2-6B:', count_params(model.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8bdef4-3522-422e-9b25-a6d74f8e5d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (embedding): Embedding(\n",
       "      (word_embeddings): Embedding(65024, 4096)\n",
       "    )\n",
       "    (rotary_pos_emb): RotaryEmbedding()\n",
       "    (encoder): GLMTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x GLMBlock(\n",
       "          (input_layernorm): RMSNorm()\n",
       "          (self_attention): SelfAttention(\n",
       "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n",
       "            (core_attention): CoreAttention(\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNorm()\n",
       "          (mlp): MLP(\n",
       "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
       "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layernorm): RMSNorm()\n",
       "    )\n",
       "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
